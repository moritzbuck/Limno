
import os
from glob import glob
from os.path import join as pjoin


workdir : "/home/moritz/people/0010_Pauline/"

TEMP_DIR = '/scratch/'
TRIMMOMATIC_HOME = '/sw/apps/bioinfo/trimmomatic/0.36/rackham/'
COAS_FILES_DIR = '/home/moritz/people/0010_Pauline/0000_raws/0200_coasses/'
TRIMMOMATIC_JAR_PROGRAM = "trimmomatic.jar"
THREADS = 20
MAX_MEM = '128g'
sortrna_db  = ['rfam-5.8s-database-id98', 'rfam-5s-database-id98', 'silva-arc-16s-id95', 'silva-arc-23s-id98', 'silva-bac-16s-id90', 'silva-bac-23s-id98', 'silva-euk-18s-id95', 'silva-euk-28s-id98']
sortrna_db_loc = '/sw/apps/bioinfo/SortMeRNA/2.1b/rackham/sortmerna'
" ".join([ pjoin(sortrna_db_loc,'rRNA_databases',d+".fasta") + "," + pjoin(sortrna_db_loc,'index',d) for d in sortrna_db])

trimmomatic_config = {
    	'options' : "-phred33",
        'processing_options' : "ILLUMINACLIP:" + TRIMMOMATIC_HOME + "/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36",
        'cmd' : "java -Xmx" + MAX_MEM + " -Djava.io.tmpdir=" + TEMP_DIR +  " -jar " + os.path.join(TRIMMOMATIC_HOME, TRIMMOMATIC_JAR_PROGRAM) + " PE ",
}

def get_coas_libs(wildcards):
    coas_name = wildcards.name
    with open(pjoin(COAS_FILES_DIR, coas_name + ".txt")) as handle:
        samples = [l.strip() for l in handle]
    fwds = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz".format(sample = s) for s in samples]
    revs = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz".format(sample = s) for s in samples]
    u_fwds = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz".format(sample = s) for s in samples]
    u_revs = ["1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz".format(sample = s) for s in samples]
    return {'fwd' : fwds, 'rev' : revs, 'u_rev' : u_revs, 'u_fwd' : u_fwds}

def find_fastq(wildcards):
    path = '0000_raws/0100_reads/{id}/'.format(id=wildcards.sample)
    result = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.fastq.gz'))]
    return result

def all_samples():
    path = "1000_processed_reads/"
    samples = [d for d in os.listdir(path) if os.path.isdir(pjoin(path,d)) ]
    return samples

def all_kaiju(wildcards):
    return ["1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.out.summary".format(sample=f) for f in all_samples()]

def all_dones(wildcards):
    path = '0000_raws/0100_reads/'
    result = [pjoin("1000_processed_reads/",s,"done") for s in os.listdir(path) if os.path.isdir(pjoin(path,s)) ]
    return result

# rule all:
#     """ Generate all """
#     input : 'big_table.csv'


rule fastqc:
    input :  find_fastq
    output : "1000_processed_reads/genomic/{sample}/reads/fastqc"
    threads : THREADS
    shell:
        """
        fastqc -t {threads} -o {output} {input}
        """


rule trimmomatic:
    """ QCing and cleaning reads """
    params : cmd = trimmomatic_config['cmd'],
             options = trimmomatic_config['options'],
             processing_options = trimmomatic_config['processing_options'],
             temp_folder = TEMP_DIR
    input :  find_fastq
    output : read1 = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
             read2 = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz",
             read1U = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz",
             read2U = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz",
    threads : THREADS
    log : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}.log"
    shell:
        """
        unpigz -c `echo {input} | tr ' ' '\n' | grep "_R1_"`  >  {params.temp_folder}/temp_R1.fastq
        unpigz -c `echo {input} | tr ' ' '\n' | grep "_R2_"` >  {params.temp_folder}/temp_R2.fastq

        {params.cmd} {params.options} {params.temp_folder}/temp_R1.fastq {params.temp_folder}/temp_R2.fastq -threads {threads} -baseout {params.temp_folder}/{wildcards.sample}.fastq.gz {params.processing_options} 2> {log}
        mv {params.temp_folder}/{wildcards.sample}*.fastq.gz 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/
        """

rule mash:
    params : kmer = 21,
             hashes = 1000
    input : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz","1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz"
    output : "1000_processed_reads/{sample}/reads/mash/{sample}.msh"
    log : "1000_processed_reads/{sample}/reads/mash/{sample}.log"
    threads : THREADS
    shell :
        "mash sketch -r -p {threads} -k {params.kmer} -s {params.hashes} -o $(echo '{output}' | sed -e 's/.msh//') {input} > {log}"

rule kaiju:
    params : db_path = "/crex2/proj/sllstore2017039/2017_MG_course/data/kaijudb/kaiju_nr_db/",
             db = "kaijudb.fmi"
    input : "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz","1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz"
    output : "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.out.summary", "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.html"
    log : "1000_processed_reads/{sample}/reads/kaiju/{sample}_kaiju.log"
    threads : THREADS
    shell : """
    kaiju -t {params.db_path}/nodes.dmp -f {params.db_path}/{params.db}   -i 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/{wildcards.sample}_1P.fastq.gz -j 1000_processed_reads/{wildcards.sample}/reads/trimmomatic/{wildcards.sample}_2P.fastq.gz -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -z {threads} > {log}
    kaiju2krona -u -t {params.db_path}/nodes.dmp -n {params.db_path}/names.dmp -i 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.krona >> {log}
    ktImportText  -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.html 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.krona >> {log}
    kaijuReport -p -r genus -t {params.db_path}/nodes.dmp -n {params.db_path}/names.dmp -i 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out -r family -o 1000_processed_reads/{wildcards.sample}/reads/kaiju/{wildcards.sample}_kaiju.out.summary >> {log}
    """

rule MAG_stats:
    input : bin_id = "{path}/{name}/bins/{assembler}/MAGs/{bin_id}",
            checkm = "{path}/{name}/bins/{assembler}/MAGs/{name}.checkm"
    output : "{path}/{name}/bins/{assembler}/MAGs/{bin_id}/{bin_id}.stats"
    run :
        from bio import SeqIO

        out_dict = {}
        bin_path = input.bin_id
        checkm_file = input.checkm
        out_file = output[0]
        bin_id = wildcards.bin_id
        with open("phylophlan_tax.txt") as handle:
            tax = [l.split()[:2] for l in handle.readlines()]
            tax = {t[0] : ":".join([tt for tt in t[1].split(".") if "?" not in tt ]) for t in tax if "all_samples-" in t[0]}
        def process_bin(binl) :
            bin_path = binl
            bin_id = binl
            out_dict = {}

            checkm_fields = ['Bin Id', 'Marker lineage', 'UID', '# genomes', '# markers', '# marker sets', '0', '1', '2', '3', '4', '5+', 'Completeness', 'Contamination', 'Strain heterogeneity']
            with open(checkm_file) as handle:
                dat = handle.readlines()
            dat = {l.split()[0] : {k : v for k,v in zip(checkm_fields, l[:-1].split() ) } for l in dat[3:-1]}
            checkm_dat = dat[bin_id]
            out_dict['completeness'] = checkm_dat['Completeness']
            out_dict['contamination'] = checkm_dat['Contamination']
            out_dict['taxo:checkm'] = checkm_dat['Marker lineage']
            out_dict['strain_heterogeneity'] = checkm_dat['Strain heterogeneity']

            with open( bin_path + "/" + bin_id + ".fna") as handle:
                fna = [s for s in SeqIO.parse(handle, "fasta")]
            with open( bin_path + "/" + bin_id + ".faa") as handle:
                faa = [s for s in SeqIO.parse(handle, "fasta")]

            out_dict['length'] = sum([len(s.seq) for s in fna])
            out_dict['nb_contigs'] = len(fna)
            out_dict['nb_proteins'] = len(faa)
            out_dict['coding_density'] = (3.0*sum([len(s.seq) for s in faa]))/sum([len(s.seq) for s in fna])
            out_dict['GC'] = float(sum([str(s.seq).count("G")+str(s.seq).count("C") for s in fna]))/out_dict['length']
            out_dict['taxo:phylophlan'] = tax[bin_id]
            return out_dict

        DataFrame.from_dict(dat, orient = 'index').to_csv("bin_stats.csv")

# rule blast_db:
#     input : "{path}/{name}/bins/{assembler}/MAGs/{bin_id}"
#     output : "{path}/{name}/bins/{assembler}/MAGs/{bin_id}/{bin_id}_{db}.json"
#     threads : THREADS
#     run :
#         from Bio import SeqIO
#         import os
#         from os.path import join as pjoin
#         import pandas
#         from pandas import Index
#         from tqdm import tqdm
#
#         blast_line = 'tblastx -db ~/data/dbs/{db}/*.fna -query {fasta} -outfmt 6 -num_threads {threads} -out {blast_out}'
#         head =['query', 'subject', 'identity', 'length', 'mismatch', 'gaps', 'qstart', 'qend', 'sstart','send','evalue','bitscore']
#
#         infasta = pjoin(input, wildcards.bin_id + ".ffn")
#         db_path = "/home/moritz/data/dbs/{db}/".format(db = wildcards.db)
#         tax_file = pjoin(db_path, "taxonomy.tsv")
#         blastout = pjoin(input, wildcards.bin_id + "_" + wildcards.db + ".blast"
#
#         os.system(blast_line.format(db = wildcards.db, fasta = infasta, threads = threads, blast_out = blastout)
#
#         with open(tax_file) as handle:
#             tax_dir = {l.split("\t")[0] : "".join(l[:-1].split("\t")[-2:]) for l in tqdm(handle)}
#
#         raw_data = pandas.read_csv(blastout, header = -1, index_col = False, sep="\t")
#         raw_data.columns = Index(head)
#
#         with open(infasta) as handle:
#             seqs = {s.id : s.seq for s in SeqIO.parse(handle,"fasta")}


rule clean_MAGs:
    input : "{path}/{name}/bins/{assembler}/{name}"
    output : "{path}/{name}/bins/{assembler}/MAGs"
    run :
        import os
        from Bio import SeqIO
        from os.path import join as pjoin
        from tqdm import tqdm

        ipath = pjoin(os.path.dirname(input[0]), "dirty_bins/")
        opath = output[0]
        os.makedirs(opath)
        for f in tqdm(os.listdir(ipath)):
            if f[-3:] == ".fa":
                with open(pjoin(ipath, f)) as handle:
                    seqs = [s for s in SeqIO.parse(handle, "fasta")]
                zeros = len(str(len(seqs)))
                bin_name = f[:-3]
                for i,s in enumerate(seqs):
                    s.id = bin_name.replace(".","-") + "-" + str(i+1).zfill(zeros)
                    s.description = ""
                with open(pjoin(opath, f[:-3].replace(".","-")+".fa"), "w") as handle:
                    SeqIO.write(seqs, handle, "fasta")

rule all_kaiju:
    input : all_kaiju
    output : "1000_processed_reads/kaiju_table.tsv"
    shell : """
    from ete3 import NCBITaxa
    from collections import defaultdict
    import os
    from tqdm import tqdm
    from os.path import join as pjoin
    from pandas import DataFrame, concat

    out_file="1000_processed_reads/kaiju_table.tsv"

    pat = "/home/moritz/people/0010_Pauline/1000_processed_reads/"
    samples = [s for s in os.listdir(pat) if "P6404" in s]
    out_dict = {}

    for s in tqdm(samples):
        if not out_dict.get(s):
            out_dict[s]=defaultdict(int)
            with open(pjoin(pat, s, "reads", "kaiju", s+"_kaiju.out")) as handle:
                for l in tqdm(handle):
                    out_dict[s][l.split()[2]]+=1

    data = DataFrame.from_dict(out_dict)
    data = data.fillna(0)

    taxDb = NCBITaxa()
    line= {i : taxDb.get_lineage(i) for i in tqdm(list(data.index))}
    out = {i : taxDb.get_taxid_translator(l) if l else None for i,l in tqdm(line.items())}
    tt = sum([list(l.keys()) for l in tqdm(out.values()) if l], [])
    tt = list(set(tt))
    tt = taxDb.get_rank(tt)

    out_l = {k : {tt[k] : v for k,v in t.items()} if t else None for k,t in tqdm(out.items())}
    taxos = DataFrame.from_dict(out_l).transpose()
    taxos = taxos.fillna("NA")
    all = concat([data, taxos.loc[data.index]], axis=1)
"""

rule megahit_single:
    params : temp_folder = TEMP_DIR
    input : fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
            rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2P.fastq.gz",
            u_rev = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_2U.fastq.gz",
            u_fwd = "1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1U.fastq.gz"
    output : assembly = "1000_processed_reads/{sample}/assemblies/megahit/{sample}.fna",
             folder = "1000_processed_reads/{sample}/assemblies/megahit/data"
    threads : THREADS
    shell : """
        unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
        unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
        unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
        unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        megahit  -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -r {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data --out-prefix megahit --continue
        rm -r {params.temp_folder}/temp_data/intermediate_contigs/
        mv {params.temp_folder}/temp_data/ {output.folder}
        cp 1000_processed_reads/{wildcards.sample}/assemblies/megahit/data/megahit.contigs.fa {output.assembly}
    """

rule megahit_coas:
    params : temp_folder = TEMP_DIR
    input : unpack(get_coas_libs)
    output :    folder = "1500_assemblies/{name}/megahit/data/",
                assembly = "1500_assemblies/{name}/megahit/{name}.fna",
    threads : THREADS
    shell : """
         unpigz -c {input.fwd}  >  {params.temp_folder}/temp_R1.fastq
         unpigz -c {input.rev}  >  {params.temp_folder}/temp_R2.fastq
         unpigz -c {input.u_rev}  >  {params.temp_folder}/temp_U.fastq
         unpigz -c {input.u_fwd}  >>  {params.temp_folder}/temp_U.fastq
        megahit  -1 {params.temp_folder}/temp_R1.fastq -2 {params.temp_folder}/temp_R2.fastq -r {params.temp_folder}/temp_U.fastq -t {threads} -o {params.temp_folder}/temp_data --out-prefix megahit --continue
        rm -r {params.temp_folder}/temp_data/intermediate_contigs/
        mv {params.temp_folder}/temp_data/ {output.folder}
        cp 1000_processed_reads/{wildcards.name}/assemblies/megahit/data/megahit.contigs.fa {output.assembly}
    """

rule bbmap_index:
    input : "{path}"
    output : folder = "{path}/mapping"
    shell : """
    module load bioinfo-tools
    module load bbmap

    genome=`ls {input}/*.fna`
    bbmap.sh ref=$genome path={wildcards.path}/mapping
    """

rule bbmap_all_samples:
    params : home = "/home/moritz/people/0010_Pauline/1000_processed_reads"
    input : "{path}/mapping"
    output : "{path}/mapping/map_table.tsv",
    	     "{path}/mapping/bams/",
    threads : THREADS
    shell : """
        module load bioinfo-tools
        module load bbmap
        module load samtools

        cd {wildcards.path}/mapping

        for s in `echo """ + " ".join(all_samples()) + """ | tr ' ' "\n"`
        do

            base={params.home}/$s/reads/trimmomatic/$s
            bbmap.sh  in=${{base}}_1P.fastq.gz in2=${{base}}_2P.fastq.gz threads={threads} bamscript=bamscript.sh out=bams/$s.sam
            ./bamscript.sh

            sambamba flagstat -t {threads} bams/${{s}}_sorted.bam > bams/${{s}}_sorted.stats
            sambamba markdup --hash-table-size=4194304 --io-buffer-size=8000 --overflow-list-size=4194304 -r -t {threads}
            samtools rmdup  bams/${{s}}_sorted.bam bams/$s.bam
            samtools index bams/$s.bam
            sambamba flagstat  -t {threads} bams/${{s}}.bam > bams/${{s}}.stats

            rm bams/*.sam
            rm bams/*_sorted.bam*

        done
    jgi_summarize_bam_contig_depths --outputDepth map_table.tsv  --pairedContigs paired_contigs.tsv  bams/*.bam
    rm bamscript.sh
    rm bams/*.bam
    """

rule metabat :
    input : mapping = "{path}/{name}/{assembler}/mapping/map_table.tsv"
    output : "{path}/{name}/bins/{assembler}/{name}",
             "{path}/{name}/bins/{assembler}/dirty_bins/unbinned/{name}.unbinned.fa"
    threads : THREADS
    shell : """
    OPATH={wildcards.path}/{wildcards.name}/bins/{wildcards.assembler}/
    metabat2  -i {wildcards.path}/{wildcards.name}/{wildcards.assembler}/{wildcards.name}.fna  -o $OPATH/{wildcards.name} -a {wildcards.path}/{wildcards.name}/{wildcards.assembler}/mapping/map_table.tsv  --saveCls  --unbinned -t {threads}
    if [ -f $OPATH/dirty_bins/{wildcards.name}.unbinned.fa ]
    then
        mv $OPATH/*.fa $OPATH/dirty_bins/
        mv $OPATH/dirty_bins/{wildcards.name}.unbinned.fa $OPATH/dirty_bins/unbinned/
    else
        touch $OPATH/{wildcards.name}
        cp {wildcards.path}/{wildcards.name}/{wildcards.assembler}/{wildcards.name}.fna $OPATH/dirty_bins/unbinned/{wildcards.name}.unbinned.fa
    fi
    """

rule checkm :
    input : "{path}/{name}/bins/{assembler}/{type}MAGs"
    output : "{path}/{name}/bins/{assembler}/{type}MAGs/{name}.checkm"
    threads : THREADS
    shell : """
    checkm lineage_wf -t {threads} -x fa {input} {input}/data > {output}
    """

rule phylophlan :
    params : phylophlan_path = "/home/moritz/repos/github/phylophlan",
             phylophlan_exe = "phylophlan.py",
             phylophlan_taxa = "data/ppafull.tax.txt",
             default_genomes = "/home/moritz/repos/github/phylophlan/input/default_genomes"
    input : path = "{path}/{name}/bins/{assembler}/{type}MAGs", annotated = "{path}/{name}/bins/{assembler}/annotated"
    output : "{path}/{name}/bins/{assembler}/{type}{name}.tree"
    threads : THREADS
    shell : """
    DD=`pwd`
    mkdir {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    cp {input.path}/*/*.faa {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    cd {params.phylophlan_path}/input/{wildcards.type}{wildcards.name}
    for f in `ls *.faa`
    do
        sed -i 's/*//g' $f
    done
    cp {params.default_genomes}/*.faa .
    cd ../..
    python2.7 phylophlan.py --nproc {threads}  -i -t  {wildcards.type}{wildcards.name}
    IFS=$"\n"; for r in `cat data/ppafull.tax.txt`; do id=`echo ${{r}} | cut -f1`; tax=`echo ${{r}} | cut -f2`; sed -i "s/${{id}}/${{id}}_${{tax}}/g" output/{wildcards.type}{wildcards.name}/{wildcards.type}{wildcards.name}.tree.int.nwk; done; unset IFS
    cp output/{wildcards.type}{wildcards.name}/{wildcards.type}{wildcards.name}.tree.int.nwk $DD/{output}
    """


rule annotate :
    input : "{path}/{name}/bins/{assembler}/MAGs"
    output : "{path}/{name}/bins/{assembler}/annotated"
    threads : THREADS
    shell : """
DIR={wildcards.path}/{wildcards.name}/bins/{wildcards.assembler}/

for f in `ls $DIR/MAGs/ | grep ".fa$"`;
do
    bin_id=`basename ${{f%%.fa}}  | cut -d"-" -f2`;
    sample=`basename $f | cut -d"-" -f1`;
    echo ${{sample}}-${{bin_id}};
    if [ ! -f $DIR/${{sample}}-${{bin_id}}/${{sample}}-${{bin_id}}.faa ]
    then
        prokka --outdir $DIR/MAGs/${{sample}}-${{bin_id}}  --force --prefix ${{sample}}-${{bin_id}} --locustag ${{sample}}-${{bin_id}} --cpus {threads} $DIR/MAGs/$f 2> /dev/null ;
        echo bla
    fi
done
touch $DIR/annotated
#prokka --metagenome --outdir $DIR/MAGs/${{sample}}-${{bin_id}}  --force --prefix ${{sample}}-${{bin_id}} --locustag ${{sample}}-${{bin_id}} --cpus {threads} $DIR/MAGs/$f 2> /dev/null ;
"""

rule filter_good_MAGs :
# cutoff based on https://www.microbe.net/2017/12/13/why-genome-completeness-and-contamination-estimates-are-more-complicated-than-you-think/
    params : contamination = 5, completeness = 40
    input : path = "{path}/{name}/bins/{assembler}/MAGs",
            anots = "{path}/{name}/bins/{assembler}/annotated",
            checkm = "{path}/{name}/bins/{assembler}/MAGs/{name}.checkm"
    output : "{path}/{name}/bins/{assembler}/good_MAGs"
    run :
        from os.path import join as pjoin
        import os
        import shutil

        checkm_fields = ['Bin Id', 'Marker lineage', 'UID', '# genomes', '# markers', '# marker sets', '0', '1', '2', '3', '4', '5+', 'Completeness', 'Contamination', 'Strain heterogeneity']
        with open(input.checkm) as handle:
            dat = handle.readlines()
        dat = {l.split()[0] : {k : v for k,v in zip(checkm_fields, l[:-1].split() ) } for l in dat[3:-1]}
        good_bact_MAGs = [k for k,v in dat.items() if float(v['Completeness']) > params.completeness and float(v['Contamination']) < params.contamination]

        os.makedirs(output[0])
        for f in good_bact_MAGs:
            shutil.copytree(pjoin(input.path, f), pjoin(output[0],f))

rule library:
    input : #"1000_processed_reads/{sample}/reads/trimmomatic/{sample}_1P.fastq.gz",
            "1000_processed_reads/{sample}/reads/mash/{sample}.msh",
            "1000_processed_reads/{sample}/assemblies/megahit/mapping/map_table.tsv",
    output : "1000_processed_reads/{sample}/done"
    shell:
        "touch 1000_processed_reads/{wildcards.sample}/done"



rule all_libraries :
    input : all_dones, "1000_processed_reads/kaiju_table.tsv"
#    output : "1000_processed_reads/done"
#    shell : "touch 1000_processed_reads/done"
